{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58c13c0",
   "metadata": {},
   "source": [
    "### Continual CartPole-1 Training Notebook v1\n",
    "\n",
    "Contains the code and methodology to train a continual-learning CartPole model using a DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959eda82",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe3ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba579def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0222b271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x249768cc4f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3d281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Assign the device to CPU or GPU if available\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    'cpu'\n",
    ")\n",
    "\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e541f",
   "metadata": {},
   "source": [
    "#### Replay Memory\n",
    "\n",
    "Replay memory stores past 'Transition' states into a buffer of 10,000 of the latest experiences. These transitions are used to optimize the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a5ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Creates a deque object storing type array with a capacity\n",
    "        # Once capacity is reached, old experiences will automatically be overwriteen with the new\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    # Save the transition to the memory\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    # Return batch_size samples for use in optimization \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901b7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model used\n",
    "from CartPole_DQN_model import CartPoleModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62079ec5",
   "metadata": {},
   "source": [
    "#### Task Creation\n",
    "\n",
    "This function will create x amount of tasks and return a gymnasium env variable which will be used during continual learning on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7283afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Later on add velocity and the other one for more complex changes\n",
    "def create_env(length=0.5, gravity=9.8, masspole=0.1, masscart=1.0, force_mag=10.0):\n",
    "\n",
    "    temp_env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # Update the length of pole (randomize later)\n",
    "    temp_env.unwrapped.length = length\n",
    "    temp_env.unwrapped.gravity = gravity\n",
    "    temp_env.unwrapped.masspole = masspole\n",
    "    temp_env.unwrapped.masscart = masscart\n",
    "    temp_env.unwrapped.force_mag = force_mag\n",
    "    \n",
    "    return temp_env\n",
    "    \n",
    "# Create 3 discrete tasks\n",
    "envs = [\n",
    "    create_env(), # Default env\n",
    "    create_env(0.75, 5, 2, 2, 5), # shorter pole and heavier gravity\n",
    "    create_env(0.25, 20, 0.05, 0.5, 20) # Shorted pole and less gravity\n",
    "]\n",
    "\n",
    "default_env = envs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a6c88",
   "metadata": {},
   "source": [
    "#### Pre-Training Setup\n",
    "\n",
    "Define variables needed during training. Initialize the memory buffer, and policy and target networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd22bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99 # Discount factor on future states\n",
    "\n",
    "# Epsilon-greedy variables needed\n",
    "EPS_START = 0.9 # Start off encouraging the model to explore\n",
    "EPS_END = 0.05 # After exploring, the model will stabilize and rarely explore\n",
    "EPS_DECAY = 1000 # Rate at which the epsilon threshold will decrease (exponent)\n",
    "\n",
    "TAU = 0.005 # Rate at which the target network will be updated\n",
    "LR = 1e-4 # Learning rate of the policy net\n",
    "\n",
    "# Retrieve the number of actions possible in the environment (2 - Left or Right)\n",
    "n_actions = default_env.action_space.n\n",
    "\n",
    "# Initialize the env state and store as variable\n",
    "state, _ = default_env.reset()\n",
    "\n",
    "# Store the number of variables inside the observation space\n",
    "# This means we store each input variable being passed into the model\n",
    "# In CartPole-v1 there are 4 (position, velocity, and 2 more idk)\n",
    "n_observations = len(state)\n",
    "\n",
    "# Number of nodes in each layer of the neural net\n",
    "n_nodes = 256\n",
    "\n",
    "# Instantiate both the policy and target net\n",
    "policy_network = CartPoleModel(n_observations, n_actions, n_nodes)\n",
    "target_network = CartPoleModel(n_observations, n_actions, n_nodes)\n",
    "\n",
    "# Set the target network weights to the same as policy net \n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "# Send models to the GPU (or CPU)\n",
    "policy_network.to(device)\n",
    "target_network.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(policy_network.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# Create the Experience replay buffer with a cap of 10,000\n",
    "memory = ReplayMemory(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69dc886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the method that selects an e-greedy action\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state, eps_greedy=True):\n",
    "    global steps_done\n",
    "\n",
    "    # During evaulation we don't want e-greedy, we want just greedy\n",
    "    if not eps_greedy:\n",
    "        with torch.no_grad():\n",
    "            return policy_network(state).max(1).indices.view(1, 1)\n",
    "\n",
    "    # Random number\n",
    "    # If number is above eps_threshold, select greedy action (highest q-value)\n",
    "    # Otherwise choose a random action to explore\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "    # Greedy select here\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # PAss the state through the model without gradient descent (we're not learning here, just need an action)\n",
    "            # Select the max Q-Value (action) the model chose and return the reshaped index of said action\n",
    "            return policy_network(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        # If random action is selected, choose a random action from env, and send to device\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7173f92e",
   "metadata": {},
   "source": [
    "#### Plotting Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bc49780",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "# def plot_durations(show_result=False):\n",
    "#     plt.figure(1)\n",
    "#     durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "#     if show_result:\n",
    "#         plt.title('Result')\n",
    "#     else:\n",
    "#         plt.clf() # Clear current figure\n",
    "#         plt.title('Training...')\n",
    "\n",
    "#     plt.xlabel('Episode')\n",
    "#     plt.ylabel('Duration')\n",
    "#     plt.plot(durations_t.numpy())\n",
    "\n",
    "#     # Additionally, start plotting the mean after 100 episodes\n",
    "#     if len(durations_t) >= 100:\n",
    "#         means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "#         means = torch.cat((torch.zeros(99), means))\n",
    "#         plt.plot(means.numpy())\n",
    "\n",
    "#     plt.pause(0.001)\n",
    "#     if is_ipython:\n",
    "#         if not show_result:\n",
    "#             display.display(plt.gcf())\n",
    "#             display.clear_output(wait=True)\n",
    "#         else:\n",
    "#             display.display(plt.gcf())\n",
    "\n",
    "\n",
    "def plot_task_evals(eval_checkpoints, task_eval_history):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title(\"Continual Learning Task Performance\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "\n",
    "    for task_idx, scores in enumerate(task_eval_history):\n",
    "        # Replace None with np.nan for plotting gaps\n",
    "        y_vals = [s if s is not None else np.nan for s in scores]\n",
    "        plt.plot(eval_checkpoints, y_vals, label=f\"Task {task_idx}\", linewidth=2)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78520c8a",
   "metadata": {},
   "source": [
    "#### Model Optimization\n",
    "\n",
    "This is where the learning of the policy_network occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da8977a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "\n",
    "    # If we don't have BATCH_SIZE number of experiences yet\n",
    "    # We can't optimize the model, so skip until false\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # Select a BATCH_SIZE number of experiences (at random) from memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # Reshape array of tuple Transitions into array of tensor transitions\n",
    "    # Combine into a single Transition capable of being batch processed\n",
    "    # I don't really get this line. Just reshapes and redefines\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Now we store a mask of all the non-terminal states within the batch\n",
    "    non_terminal_mask = torch.tensor(\n",
    "        # For every state, if it's not terminal, return the next_state\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device,\n",
    "        dtype=torch.bool\n",
    "    )\n",
    "\n",
    "    # Extract all the non-terminal next_states from the batch\n",
    "    non_terminal_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    # Store the state, action, and reward of each item in the batch\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Forward pass the policy_network to see what actions the model thinks should be taken\n",
    "    # Take the q-value for what action actually was taken (based on the experience)\n",
    "    # gather(1, action_batch) just matches index of action to index of q-value of network output \n",
    "    state_action_values = policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # For non-terminal states, we need to forward pass the target model to see what the future q-value will be\n",
    "    next_states_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        # forward pass the target model and take the MAX Q-Value\n",
    "        # Only do this for next_states that don't terminate\n",
    "        next_states_values[non_terminal_mask] = target_network(non_terminal_next_states).max(1).values\n",
    "    \n",
    "    # Bellman equation\n",
    "    expected_state_action_values = (next_states_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Calculate the Huber Loss (acts like MSE when low, and MAE when high for stability)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimze model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_network.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc9f4c5",
   "metadata": {},
   "source": [
    "#### Model Evaulation Method\n",
    "\n",
    "This will be run at the end of every x episodes on every task to gague how the model is doing across all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc8bf7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(envs, num_episodes=5):\n",
    "\n",
    "    # Keep track of how much the model earns on this environment\n",
    "    envs_total_reward = []\n",
    "    \n",
    "    # Loop over every environment\n",
    "    for env in envs:\n",
    "\n",
    "        current_env_total_reward = []\n",
    "        for i_episode in range(num_episodes):\n",
    "\n",
    "            # Reset the current environment and store state\n",
    "            state, _ = env.reset()\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            current_episode_reward = 0\n",
    "\n",
    "            # Play through the episode updating current_episode_reward each time\n",
    "            done = False\n",
    "            while not done:\n",
    "                \n",
    "                # Forward pass the model to get the action\n",
    "                # eps_greedy = False because we want just the greedy action, nothing with epsilon\n",
    "                action = select_action(state, eps_greedy=False)\n",
    "\n",
    "                # Apply the action to the state and get new state along with reward and info\n",
    "                state, reward, terminated, trunc, _ = env.step(action.item())\n",
    "\n",
    "                # Convert to a tensor again\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                current_episode_reward += reward\n",
    "\n",
    "                done = terminated or trunc\n",
    "\n",
    "            current_env_total_reward.append(current_episode_reward)\n",
    "\n",
    "        # Append the average of this current tasks reward\n",
    "        envs_total_reward.append(sum(current_env_total_reward) / num_episodes)\n",
    "    \n",
    "    # Return the list of rewards for each task\n",
    "    return envs_total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bdeb4",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cd9942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "# After how many episodes do we evaluate performance on all tasks\n",
    "eval_rate = 20\n",
    "total_evaluation = []\n",
    "\n",
    "num_tasks = len(envs)\n",
    "task_eval_history = [[] for _ in range(num_tasks)]\n",
    "eval_checkpoints = []  # X-axis: episode counts or steps\n",
    "global_episodes = 0\n",
    "\n",
    "# Loop over every task (environment) and run for num_episodes\n",
    "for env in envs:\n",
    "    for i_episode in range(num_episodes):\n",
    "        global_episodes += 1\n",
    "        # Reset env and store the state\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # count(1) means start the count at 1 instead of 0\n",
    "        for t in count(1):\n",
    "            # Select the action to take (e-greedy)\n",
    "            action = select_action(state)\n",
    "            observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Update the next_state\n",
    "            # Convert to tensor as prep for model \n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            \n",
    "            # Save the current transition into memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "\n",
    "            # Apply model optimization (Bellman)\n",
    "            optimize_model()\n",
    "\n",
    "            # Soft update the target network\n",
    "            target_net_state_dict = target_network.state_dict()\n",
    "            policy_net_state_dict = policy_network.state_dict()\n",
    "\n",
    "            # Take the weights of the current target net and nudge them toward the policy net very slowly\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key] * TAU \\\n",
    "                    + target_net_state_dict[key] * (1 - TAU)\n",
    "            \n",
    "            target_network.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                episode_durations.append(t)\n",
    "                # plot_durations()\n",
    "                break\n",
    "        \n",
    "        # Every episode_eval_rate evaluate the model on all tasks\n",
    "        if i_episode % eval_rate == 0:\n",
    "            current_eval = model_eval(envs=envs, num_episodes=5)\n",
    "            print(f'Current evaluation on all tasks: {current_eval}')\n",
    "            total_evaluation.append(current_eval)\n",
    "\n",
    "            # Assume: current_task_index is the index of the task currently being trained on\n",
    "            eval_checkpoints.append(global_episodes)  # could also use steps\n",
    "\n",
    "            # current_eval only contains evaluations for tasks seen so far\n",
    "            for task_idx in range(num_tasks):\n",
    "                if task_idx < len(current_eval):\n",
    "                    task_eval_history[task_idx].append(current_eval[task_idx])\n",
    "                else:\n",
    "                    task_eval_history[task_idx].append(None)  # pad for tasks not yet trained/evaluated\n",
    "\n",
    "            plot_task_evals(eval_checkpoints, task_eval_history)\n",
    "\n",
    "\n",
    "print('Complete')\n",
    "# plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "261a03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for simulation in other .py file\n",
    "torch.save(policy_network.state_dict(), './models/continual_policy_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b462ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This will save as a video as mp4\n",
    "# env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "# env = gym.wrappers.RecordVideo(env, video_folder='./videos', episode_trigger=lambda ep: True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
